{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling and statistics used\n",
    "\n",
    "Throughout this week you'll be using various models and metrics available in both keras and sklearn.\n",
    "\n",
    "To used to some of the most common ones we'll be using, have a look through the following:\n",
    "1. Training, validation and test sets\n",
    "2. Cross Validation\n",
    "3. Model selection, fitting and predicting\n",
    "4. Mean squared error, R-squared\n",
    "\n",
    "    \n",
    "## Data \n",
    "\n",
    "To get you to grips with this, we'll use a dataset readily available in [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes = datasets.load_diabetes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try and make sense of the data:\n",
    "\n",
    "1. What is diabetes[\"data\"] referring to?\n",
    "2. What is the diabetes[\"target\"]?\n",
    "3. What are the different features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make this much easier to visualise, we can put these into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(diabetes[\"data\"]) #turn array into a pandas dataframe\n",
    "data.columns = diabetes[\"feature_names\"] #save the column names\n",
    "data[\"Y\"] = diabetes[\"target\"] #save the target variable\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there negative values? It may help to see the DESCR from diabetes. \n",
    "\n",
    "How many entries are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = None #replace None with the correct code\n",
    "print(\"There are %i entries.\" %entries) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test sets\n",
    "\n",
    "When modelling, we need to split our data into sections. \n",
    "\n",
    "* A <b>training</b> set, for which the model is trained on and can make predictions using.\n",
    "* A <b>validation</b> set, for which the model can be optimised against. Once this is done, the validation set can be reincorporated into the training set.\n",
    "* A <b>test</b> set, completely excluded from training and optimisation, the performance of the model on this set gives indication of real-world performance.\n",
    "\n",
    "SKlearn has a nice function available to allow us to [split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) our data into train and test sets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using help() or the wiki page linked, split the 'data' dataset into 2/3 training and 1/3 test.\n",
    "\n",
    "This split is performed randomly, so unless you set a seed, it will be different each time.\n",
    "\n",
    "These are saved to two new variables, we'll call training, and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = train_test_split(None) #replace None with the appropriate arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training size = %i\" %len(training))\n",
    "print(\"Test size = %i\" %len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Let's have a go at creating a model for diabetes using just the 'bmi' feature.\n",
    "\n",
    "To start simply, we'll go with an ordinary least squares (OLS) [model](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression).\n",
    "\n",
    "Perform the following:\n",
    "\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None #replace None with the correct code\n",
    "y = None #replace None with the correct code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if we wanted to use multiple features, we can just supply them as 'x':\n",
    "    \n",
    "    x = training[[\"age\",\"sex\",\"bmi\"]]\n",
    "\n",
    "This would be <b>multiple linear regression</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train a model using:\n",
    "\n",
    "    model.fit(x,y)\n",
    "\n",
    "But this will error. As we are using only a single feature, it is reading it in as a single sample with lots of features (give it a go and look at the error, then look at 'x'). If we were performing multiple linear regression, this isn't necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore need to reshape x, using .reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As linear regression is the process of fitting a straight line: $$ Y = mX + C $$\n",
    "\n",
    "We can return the 'm' (intercept) and 'c' (bias) from the trained model.\n",
    "\n",
    "    model.coef_\n",
    "    model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Y = %0.3fX + %0.3f\" %(model.intercept_,model.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how well our model has been trained.\n",
    "\n",
    "We can compare its performance when looking at data it's already seen to see the 'training error'.\n",
    "\n",
    "As it is a regression problem, let's look at [mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html).\n",
    "\n",
    "This is the average error between the predicted and actual value across all datapoints, and is a common performance indicator for regression models.\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "\n",
    "To make a prediction, we can just use:\n",
    "\n",
    "    model.predict(x)\n",
    "    \n",
    "Don't forget to save the predictions to a new variable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_predictions = None #replace None with the correct code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(y, training_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty large, let's have a look at this in a graphic, using Matplotlib.pyplot.\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.scatter(x,y) #training data\n",
    "    plt.plot(x,predictions, color=\"green\") \n",
    "  \n",
    "plt.plot(x,y) points joined by a line. What would be the X and Y arguments here if we want to look at the predictions?\n",
    "\n",
    "You can add labels, change axes, add headers and change colors. \n",
    "\n",
    "    plt.scatter(x,y, color=\"red\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.title(\"\")\n",
    "    plt.ylim(0,500)\n",
    "    plt.xlim(0,500)\n",
    "    \n",
    "The plot can also be saved:\n",
    "    \n",
    "    plt.savefig(\"path/filename.png\", dpi=300)\n",
    "\n",
    "At the end of our plot code, we give the command:\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "If this is given before savefig, a blank plot will be saved.\n",
    "\n",
    "Feel free to add anything you wish to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#work here\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,None, linestyle=\"dashed\", color=\"green\") #replace None with the correct code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should seem familiar to when you have tried in lessons to plot lines of best fit! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "You will have noticed we haven't excluded a validation set yet. This can be done, but is generally more useful when we have a larger dataset. As we only have 294 datapoints to play with for training, we'll perform cross validation.\n",
    "\n",
    "* Hold out validation - removal of a single dataset for validation\n",
    "* Cross validation - In each of N folds, a subset of data is held out for validation and performance is measured. After all N folds, the validation performance is the average of each of the folds.\n",
    "    * N can be any integer value.\n",
    "    \n",
    "For example, in 10-fold cross validation, the algorithm is as follows:\n",
    "1. Take 90% of the data as training, exclude 10% as a validation set. \n",
    "2. Train model on the 90% training, test model performance on 10% validation set, record findings.\n",
    "3. Replace validation data into pool, take next 90% and 10%, ensuring a datapoint is only ever in a training/validation set once. Repeat 2 and 3 until all data has been used. \n",
    "\n",
    "Once again, sklearn has a nice [function](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate) for us to use rather than coding this ourselves!\n",
    "\n",
    "For regression problems in particular, it is common to use mean squared error (MSE), and the $R^2$.\n",
    "\n",
    "$$ MSE = \\frac{1}{n}\\sum{(Y_{true(i)} - Y_{pred(i)})^2} $$\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum(Y_{true(i)}-Y_{pred(i)})^2}{\\sum(Y_{true(i)} - Y_{mean})} $$\n",
    "\n",
    "Where $Y_{true(i)}$ and $Y_{pred(i)}$ are the actual and predicted Y values of the ith sample, respectively.\n",
    "\n",
    "Loosely, MSE is a reflection of the error of the model, so the smaller the score, the lower the error.\n",
    "$R^2$ is the coefficient of determination, providing a measure of how well future samples are likely to be predicted by the model. The best score is 1 - all are predicted with perfect accuracy.\n",
    "\n",
    "These can be called using functions in sklearn:\n",
    "\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    \n",
    "Or, using the 'scoring' argument in cross_validate, in which case the key word is needed: \n",
    "\n",
    "    \"r2\",\"neg_mean_squared_error\"\n",
    "    \n",
    "If you are interested, you can look at other metrics available for cross validation [here](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter), but we won't be looking at other metrics for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "#we'll reinitialize the model to make sure it isn't trained yet!\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using cross validation, we do not need to declare the 'fit' method, as it occurs behind the scenes.\n",
    "\n",
    "However, when we fully train the model, we will need to call it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3 #3 fold cross validation\n",
    "results = cross_validate(model, x.reshape(-1,1), y, cv=N, scoring=\"neg_mean_squared_error\", return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As cross validate is often using by other larger functions (for example grid search), the mean squared error is returned as a negative in order to allow ranking of parameters correctly. \n",
    "\n",
    "The actual MSE is just the absolute value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see what is returned\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to work out the average of the test scores to give a robust estimate of our model training performance.\n",
    "\n",
    "We can use numpy's functions to find the absolute values and the mean.\n",
    "\n",
    "    import numpy as np\n",
    "    np.abs()\n",
    "    np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "validation_score = np.mean(np.abs(results[\"test_score\"]))\n",
    "print(validation_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this averaged score compare to the training score you found above? \n",
    "\n",
    "Which is better? Which would you believe more and why?\n",
    "\n",
    "You could go back and see how the training score changes each time you run train_test_split without setting a seed.\n",
    "Does your CV result change much in comparison?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing a model\n",
    "\n",
    "Finally, now we have a predicted performance from validation, we can use the test set.\n",
    "\n",
    "Recall above that for testing, the model should be trained on as much data as possible, so we make sure to use the entire training dataset without cross validation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "model.fit(training[\"bmi\"].values.reshape(-1,1), training[\"Y\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = model.predict(test[\"bmi\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the results as you had above, but for the test data, and find out the mean squared error for the predictions! \n",
    "\n",
    "How does it compare to your training set?\n",
    "What is the $R^2$ for the test set?\n",
    "Is this a good model?\n",
    "\n",
    "Remember, to get MSE: \n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "(It is similar for r2_score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work here \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
