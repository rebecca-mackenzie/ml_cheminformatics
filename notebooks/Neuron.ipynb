{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Artificial Neuron\n",
    "\n",
    "\n",
    "Artifical neural networks are a connected system of \"artifical neurons\", computational elements that imitate natural nerve cells. The connection between neurons allows for signal to be tranmitted from one to the other, with the receiving neuron able to combine the input signals and generate an output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurons\n",
    "\n",
    "A single neuron can take multiple inputs, $x$, and perform a calculation to output $y$ (Figure 1). Depending on the type of the model, this may be classification or regression.\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/neuron.png\" width=\"40%\"></img>\n",
    "<b>Figure 1:</b> An artificial neuron</p>\n",
    "\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Let's imagine we are trying to predict whether a molecule is a drug (Yes|No), given its molecule fingerprint. \n",
    "\n",
    "We could use a linear model, to give the predicted output $y$:\n",
    "\n",
    "$$y = W\\cdot{x} + b$$\n",
    "\n",
    "Where $W$ is the weight vector, $x$ is the molecule fingerprint and $b$ is the bias, a real number.\n",
    "\n",
    "However, this linear model would not give values within the range 0 to 1, and indeed we wish to assess the probability that the molecule is a drug. \n",
    "\n",
    "\"What is the probability of being a drug, given $x$?\"\n",
    "\n",
    "$$y = P(y=1|x)$$\n",
    "\n",
    "We can therefore apply a function to this linear model, for example, the sigmoid function:\n",
    "\n",
    "$$z = W\\cdot{x} + b$$\n",
    "\n",
    "$$y = \\sigma(z)$$\n",
    "\n",
    "If the probability is >0.5, the prediction is a drug.\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/sigmoid.png\" width=\"50%\">\n",
    "<b> Figure 2:</b> A sigmoid function</p>\n",
    "\n",
    "For every value of z, the sigmoid function gives a value of $y$ between 0 and 1, creating an 's' shaped curve (Figure 2).\n",
    "\n",
    "The neuron is responsible for performing this calculation, but which values should be used for $W$ and $b$? Neural network modelling adopts and forward and backward approach to handle this.\n",
    "\n",
    "In the <b> feed-forward </b> pass, the inputs $x$ are taken, and the output $y$ is determined using fixed weights and biases ($W$ and $b$).\n",
    "\n",
    "Neural networks then undergo backward-propagation (gradient descent), in which a cost function is used to assess the error associated with the model using the current parameters ($\\lambda$). During each iteration, the weights and biases are updated in order to minimise the cost.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\"><b>RECAP: Neural network modelling</b><br>\n",
    "Neural network modelling typically adopts a two-stage process:<br>\n",
    "<ol>\n",
    "<li>Implementing the feed-forward pass\n",
    "    <ol>\n",
    "        <li> Taking the input and determining the output </li>\n",
    "        <li> Use fixed weights </li>\n",
    "    </ol>\n",
    "</li>\n",
    "<li>Implementing the backward propagation/gradient descent\n",
    "    <ol>\n",
    "        <li> calculate the error and gradients</li>\n",
    "        <li> update the parameters using the gradient</li>\n",
    "    </ol>\n",
    "</li>\n",
    "</ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward for a single neuron (classification)\n",
    "\n",
    "Let's do the feed forward for our single neuron using the molecule fingerprints.\n",
    "\n",
    "Note: We shall use $y$ to denote predicted output, and $\\hat{y}$ to denote actual output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/neuron_parameters.png\" width=\"50%\">\n",
    "<br><b>Figure 3:</b> Logistic regression model. </p>\n",
    "\n",
    "### Ensuring the parameter shapes are correct\n",
    "\n",
    "For a single training example:\n",
    "\n",
    "As we only have a single output, of shape $[1,1]$, working back, it means that the shape of $z$ must also be $[1,1]$:\n",
    "\n",
    "$$[1,1] = \\sigma[1,1]$$\n",
    "\n",
    "$b$ is a constant, and must match the shape of $z$, so is also of shape $[1,1]$:\n",
    "\n",
    "$$[1,1] = W \\cdot x + [1,1] $$\n",
    "\n",
    "We know from our data we have 2048 features as input (a bit fingerprint), so the shape of $x$ is $[n_{features},1]$.\n",
    "\n",
    "$$[1,1] = W \\cdot [2048,1] + [1,1] $$\n",
    "\n",
    "Using matrix multiplication, how can we get a matrix of shape $[1,1]$ when multipled by a matrix of shape $[2048,1]$?\n",
    "\n",
    "Answer: $$[1,2048] \\times [2048,1] = [1,1]$$\n",
    "\n",
    "Therefore, the shape of $W$ must be [1,2048]:\n",
    "\n",
    "$$[1,1] = [1,2048] \\cdot [2048,1] + [1,1]$$\n",
    "\n",
    "$$ z = W \\cdot x + b $$\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "<u><b> Rules for shapes of parameters</u></b>\n",
    "\n",
    "The \"weight\" parameter is a vector of the shape:\n",
    "\n",
    "$$ W_{current} = [n_{current},n_{prev}]$$\n",
    "\n",
    "The \"bias\" parameter is an array of the shape:\n",
    "\n",
    "$$b_{current} = [n_{current},1]$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$n_{current}$ is the number of units in the current layer <br>\n",
    "$n_{prev}$ is the number of units in the previous layer\n",
    "</div>\n",
    "\n",
    "\n",
    "### Initialising the parameters\n",
    "\n",
    "As we know nothing about the feature weights to start with, it is custom to initialise the weight vector using random numbers, and initialise the bias using zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other activation functions\n",
    "<br>\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>Exercise:</b> Identify the functions (1-5) from the graphs (A-E).</div>\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/functions.png\" width=\"75%\"/>\n",
    "</p></font>\n",
    "\n",
    "1) $y = 2x + 1$ <br><br>\n",
    "2) $y = \\frac{1}{1+e^{-x}}$<br><br>\n",
    "3) $y = max(0,x)$<br><br>\n",
    "4) $y = \\frac{2}{1+e^{-2x}}-1$<br><br>\n",
    "5) $y = max(x,0.1x)$<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Answers:\n",
    "# 1E - linear\n",
    "# 2C - sigmoid\n",
    "# 3B - relu\n",
    "# 4A - tan\n",
    "# 5D - leaky relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear activation function\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/linear.png\" width=\"50%\">\n",
    "    <b> Figure 5:</b> The linear activation function.<br>\n",
    "    Used for the output stage of a regressor.</p>\n",
    "    \n",
    "### The tan activation function\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/tan.png\" width=\"50%\">\n",
    "    <b> Figure 6:</b> The tan activation function</p>\n",
    "    \n",
    "### The ReLU activation function\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/relu.png\" width=\"50%\">\n",
    "    <b> Figure 7:</b> The ReLU activation function.<br>\n",
    "    A good choice for hidden layers. Gives faster training because it is easy to differentiate.</p>\n",
    "    \n",
    "    \n",
    "### The Leaky ReLU activation function\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/leakyrelu.png\" width=\"50%\">\n",
    "    <b> Figure 8:</b> The leaky ReLU activation function</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Building the neural network\n",
    "\n",
    "Now we have understood the processes using a single neuron, let's look at building a network of neurons to create our model.\n",
    "\n",
    "In a neural network, there are multiple layers:\n",
    "\n",
    "* Input layer - passes the data directly to the first hidden layer\n",
    "* Hidden layer - transforms the inputs into something that the output layer can use\n",
    "* Output layer - transforms the hidden layer activations into the scale required for the output\n",
    "\n",
    "Every neuron in a layer is able to receive information from every neuron in the layer before it, and is able to pass on information to every neuron in the layer after it, creating a densely connected network.\n",
    "\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"../img/neural_network.png\" width=\"50%\"> \n",
    "    <b>Figure 9:</b> A neural network</p>\n",
    "    \n",
    "Here, it's important to introduce some notation, subscript numbers are the training example numbers, e.g. $x_1$ is training example 1. Superscript numbers, outside of square brackets denote the feature/neuron number, e.g. $x^1$ is feature 1. Superscript numbers in square brackets refer to the layer number, whilst $h$ and $o$ refer to 'hidden' and 'output' respectively, e.g. $h^{[1]}$ is hidden layer 1, whilst $o^{[4]}$ is output layer 4. The layer numbers start at input layer being 0.\n",
    "\n",
    "In the neural network above, (Figure 9), you can see there are:\n",
    "\n",
    "* An input layer consisting of three features ($x^1$,$x^2$ and $x^3$), note we have only shown three but will be using all features in the following example.\n",
    "* 5 hidden layer 1 neurons.\n",
    "* 2 hidden layer 2 neurons.\n",
    "* 1 output layer 3 neuron, outputting a single value, $y$.\n",
    "\n",
    "In total, there are 7 neurons spread across 3 layers (we do not count the input layer), so there are 3 weight vectors, and 3 bias vectors.\n",
    "In keeping with the notation above, the weight vector and bias for the first layer would be denoted $W^{[1]}$ and $b^{[1]}$ respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "1. [Introduction to neural networks](https://www.analyticsvidhya.com/blog/2016/03/introduction-deep-learning-fundamentals-neural-networks/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
